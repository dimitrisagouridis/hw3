---
title: "hw-03"
author: "Dimitris Agouridis (S2731031)"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
## **DO NOT EDIT THIS CODE CHUNK**
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(pROC)
library(dplyr)
```

## Data load and preparation before modelling

```{r read_data}
gss16 <- read.csv("data/gss16.csv")
```

#### Cleaning and selecting columns

```{r}
gss16_advfront <- gss16 %>%
  select(advfront, emailhr, educ, polviews, wrkstat) %>%
  drop_na()
```

#### Re-levelling advfront

```{r}
gss16_advfront <- gss16_advfront %>%
  mutate(
    advfront = case_when(
      advfront == "Strongly agree" ~ "Agree",
      advfront == "Agree" ~ "Agree",
      TRUE ~ "Not agree"
    ),
    advfront = fct_relevel(advfront, "Not agree", "Agree")
  )
```

#### Re-levelling polviews

```{r}
gss16_advfront <- gss16_advfront %>%
  mutate(
    polviews = case_when(
      str_detect(polviews, "[Cc]onservative") ~ "Conservative",
      str_detect(polviews, "[Ll]iberal") ~ "Liberal",
      TRUE ~ polviews
    ),
    polviews = fct_relevel(polviews, "Conservative", "Moderate", "Liberal")
  )
```

#### Creating a new fulltime variable

```{r}
gss16_advfront <- gss16_advfront %>%
  mutate(fulltime = ifelse(wrkstat == "Working fulltime", TRUE, FALSE))
```

## Exercise 1: Create a linear regression model

#### Exercise 1 (a)

```{r}
linear_model <- lm(emailhr ~ educ + fulltime, data = gss16_advfront)
summary(linear_model)

# Visualize the data with a regression line
gss16_advfront %>%
  ggplot(aes(x = educ, y = emailhr)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  ggtitle("Linear Regression: Email Hours vs. Education")
```

*The formula for the line of best fit is: emailhr = -2.7573 + 0.6854 * educ + fulltime. This means that for every additional year of education, the expected number of hours spent on email increases by approximately 0.6854 hours. The intercept of -2.7573 suggests that without any education, the predicted email hours would be negative, which indicates that the model may not be appropriate for very low education levels. Additionally, the variable `fulltime` indicates the influence of being employed full-time on email hours, although its exact effect depends on the coefficient value.*

#### Exercise 1 (b)

```{r}
library(broom)
glance(linear_model)

# Plotting residuals to evaluate linear model assumptions
plot(linear_model, which = 1) # Residuals vs. Fitted
plot(linear_model, which = 2) # Normal Q-Q plot
```

*The R-squared value is 0.02912, indicating that the model explains only about 2.9% of the variability in email hours, which is quite low. This suggests that other factors, not included in the model, play a significant role in determining the number of hours spent on email. Despite the low R-squared value, the p-value from the F-statistic indicates that the relationship between education and email hours is statistically significant. The residual plots show some deviations from normality, which may imply that the linear model assumptions are not fully met.*

## Exercise 2: Create a workflow to fit a model

```{r split-data}
set.seed(1234)
gss16_split <- initial_split(gss16_advfront)
gss16_train <- training(gss16_split)
gss16_test  <- testing(gss16_split)
```

#### Exercise 2 (a)

```{r}
gss16_rec_1 <- recipe(advfront ~ educ, data = gss16_train) %>%
  step_dummy(all_nominal(), -all_outcomes())

gss16_mod_1 <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

gss16_wflow_1 <- workflow() %>%
  add_recipe(gss16_rec_1) %>%
  add_model(gss16_mod_1)
```

#### Exercise 2 (b)

```{r}
gss16_fit_1 <- gss16_wflow_1 %>%
  fit(data = gss16_train)

tidy(gss16_fit_1)
```

#### Exercise 2 (c)

*The logistic regression model was chosen for its suitability in analyzing binary outcomes, such as predicting whether someone agrees or does not agree with a statement. Logistic regression provides clear interpretability through the use of odds ratios, making it easy to understand the impact of each predictor. It is particularly useful for binary classification problems, where the outcome variable is categorical, as in this case. Additionally, logistic regression is computationally efficient and straightforward, making it an appropriate choice given the nature of the data.*

## Exercise 3: Logistic regression with single predictor

#### Exercise 3 (a)

```{r}
predictions <- predict(gss16_fit_1, gss16_test, type = "prob")
roc_curve <- roc(gss16_test$advfront, predictions$.pred_Agree)
plot(roc_curve, main = "ROC Curve for Model 1", col = "blue", lwd = 2)
```

*The ROC curve provides insight into the trade-off between sensitivity and specificity across different threshold values. It helps us understand how well the model is able to distinguish between those who agree and those who do not agree with the statement. A higher area under the ROC curve (AUC) indicates better model performance, with an ideal model having an AUC close to 1.*

#### Exercise 3 (b)

```{r}
cutoff_predictions <- ifelse(predictions$.pred_Agree >= 0.85, "Agree", "Not agree")
confusion_matrix <- table(cutoff_predictions, gss16_test$advfront)

sensitivity <- confusion_matrix["Agree", "Agree"] / sum(confusion_matrix[, "Agree"])
specificity <- confusion_matrix["Not agree", "Not agree"] / sum(confusion_matrix[, "Not agree"])

sensitivity
specificity
```

*The sensitivity and specificity values calculated based on a cutoff of 0.85 provide an understanding of how well the model performs in correctly identifying those who agree versus those who do not agree. Sensitivity measures the proportion of actual positives correctly identified, while specificity measures the proportion of actual negatives correctly identified. A high sensitivity and specificity would indicate that the model is effective in making accurate predictions.*

## Exercise 4: Logistic regression modelling and interpretation

#### Exercise 4 (a)

```{r}
gss16_rec_2 <- recipe(advfront ~ polviews + wrkstat + educ, data = gss16_train) %>%
  step_dummy(all_nominal(), -all_outcomes())

gss16_mod_2 <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

gss16_wflow_2 <- workflow() %>%
  add_recipe(gss16_rec_2) %>%
  add_model(gss16_mod_2)
```

#### Exercise 4 (b)

```{r}
gss16_fit_2 <- gss16_wflow_2 %>%
  fit(data = gss16_train)

predictions_2 <- predict(gss16_fit_2, gss16_test, type = "prob")
roc_curve_2 <- roc(gss16_test$advfront, predictions_2$.pred_Agree)
plot(roc_curve_2, main = "ROC Curve for Model 2", col = "red", lwd = 2)

auc_1 <- auc(roc_curve)
auc_2 <- auc(roc_curve_2)

print(paste("AUC for Model 1:", auc_1))
print(paste("AUC for Model 2:", auc_2))

if (auc_2 > auc_1) {
  message("Model 2 performs better.")
} else {
  message("Model 1 performs better.")
}
```

*The AUC values provide a measure of the overall performance of each model in distinguishing between those who agree and those who do not agree. Model 1, which only includes `educ` as a predictor, has a slightly higher AUC compared to Model 2, suggesting that adding `polviews` and `wrkstat` did not improve the model's ability to predict agreement. This could imply that `educ` is the most relevant predictor, while `polviews` and `wrkstat` may not contribute significantly to improving the predictive power.*

#### Exercise 4 (c)

*Model 1, which includes only `educ` as a predictor, has an AUC of 0.58, while Model 2, which includes `polviews`, `wrkstat`, and `educ`, has an AUC of 0.54. Despite adding more predictors, Model 2 did not improve performance. This suggests that `educ` is a stronger predictor for attitudes towards scientific research than the combination of `educ`, `polviews`, and `wrkstat`. Therefore, Model 1 is preferred due to its simplicity and slightly better predictive performance. The results indicate that the additional variables do not provide enough unique explanatory power to justify their inclusion in the model, and simpler models are often more interpretable and generalizable.*

